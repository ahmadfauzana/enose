E-NOSE COCOA BEAN CLASSIFICATION WITH MULTIPLE ML MODELS
======================================================================
Analysis started at: 2025-11-23 05:42:16
Results will be saved to: enose_run_05
======================================================================
Loading and preprocessing data...
‚úÖ Loaded training data from '3_categories (2)' sheet
‚úÖ Loaded testing data from 'unknown' sheet
Raw training data shape: (280, 15)
Raw testing data shape: (10, 15)

Training categories: ['WFB' 'UFB' 'ADB']
Number of classes: 3
Testing sample IDs: ['X1' 'X2' 'X3' 'X4' 'X5' 'X6' 'X7' 'X8' 'X9' 'X10']

=== FINAL PROCESSED DATA ===
Dataset type: 3_categories
Training data shape: (280, 15)
Testing data shape: (10, 15)
Training classes: ['WFB' 'UFB' 'ADB']
Training class distribution:
class
ADB    160
WFB     80
UFB     40
Name: count, dtype: int64
Unclassified samples to predict: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10']

==================================================
EXPLORATORY DATA ANALYSIS
==================================================

Training Data Statistics:
                ch0           ch1           ch2           ch3           ch4  ...           ch9          ch10          ch11          ch12         ch13
count    280.000000    280.000000    280.000000    280.000000    280.000000  ...    280.000000    280.000000    280.000000    280.000000   280.000000
mean   12272.649507   6797.719771  32402.189972  27770.322698  20672.928047  ...   8390.771098  10834.804831   4973.695084  29283.849513  3049.774453
std     4901.600320   5185.205904  12305.955995   5277.818615   2586.662169  ...   3392.621735   3449.741107   2382.600032  11205.990374   494.676293
min     5084.054294   2149.330500  12005.522719  16179.081820  13981.864820  ...   4002.401331   5779.140141   1239.243108   7375.124645  1968.592680
25%     8995.110880   3863.362292  23927.217773  24072.775316  18994.512013  ...   6254.773159   8763.570607   3308.385756  21690.109859  2732.249535
50%    10945.927803   5102.648086  31451.724005  26917.029187  20458.729005  ...   7253.549216   9870.847421   4356.154697  26807.964112  2994.682423
75%    13815.757665   6626.304858  37760.750470  30142.617809  22106.499015  ...   8897.995681  11509.929764   5842.586037  33021.808566  3386.057509
max    42731.830954  32783.814275  78648.664073  40549.214519  26565.280337  ...  21740.973078  24537.035018  13982.151828  65480.362365  4401.078142

[8 rows x 14 columns]

‚úÖ Class distribution plot saved to: enose_run_05/plots/01_class_distribution.png
‚úÖ Sample distribution plot saved to: enose_run_05/plots/02_sample_distribution.png
‚úÖ Correlation matrix plot saved to: enose_run_05/plots/03_correlation_matrix.png
‚úÖ Sensor boxplot saved to: enose_run_05/plots/04_sensor_boxplot.png
‚úÖ Feature importance plot saved to: enose_run_05/plots/05_feature_importance.png
‚úÖ PCA visualization plot saved to: enose_run_05/plots/06_pca_visualization.png
‚úÖ Class means comparison plot saved to: enose_run_05/plots/07_class_means_comparison.png

Training Data Class-wise Feature Analysis:
========================================

Mean sensor values by class (training data):
                ch0           ch1           ch2           ch3           ch4  ...           ch9          ch10         ch11          ch12         ch13
class                                                                        ...                                                                    
ADB    11721.332456   5489.398621  31434.848109  27492.789073  20636.270547  ...   7666.166365  10146.729843  4736.240952  28249.257743  3082.984398
UFB    21338.966778  17506.437169  49419.285942  37296.766392  24887.725502  ...  15485.155468  17560.732876  9432.844508  50116.173569  3677.237962
WFB     8842.124972   4060.003375  25828.325712  23562.168102  18638.844320  ...   6292.788379   8847.990784  3219.028636  20936.871025  2669.622807

[3 rows x 14 columns]

‚úÖ Class means saved to: enose_run_05/data/class_means.csv

Class separability analysis (ANOVA F-statistic for each feature):
ch0: F=244.32, p=7.01e-62
ch1: F=370.59, p=5.01e-79
ch2: F=77.75, p=1.59e-27
ch3: F=258.32, p=4.85e-64
ch4: F=174.91, p=7.58e-50
ch5: F=5.61, p=4.08e-03
ch6: F=427.45, p=2.14e-85
ch7: F=486.91, p=2.10e-91
ch8: F=294.13, p=3.08e-69
ch9: F=445.24, p=2.95e-87
ch10: F=272.37, p=3.91e-66
ch11: F=272.95, p=3.22e-66
ch12: F=268.21, p=1.60e-65
ch13: F=93.34, p=1.03e-31

‚úÖ ANOVA results saved to: enose_run_05/data/anova_results.csv

Feature Distribution Comparison (Training vs Unclassified Samples):
============================================================
Feature  Train Mean   Test Mean    Train Std    Test Std    
------------------------------------------------------------
ch0      12272.65     16855.74     4892.84      9045.62     
ch1      6797.72      13148.83     5175.94      11174.04    
ch2      32402.19     30214.49     12283.96     11300.89    
ch3      27770.32     32432.08     5268.39      9803.32     
ch4      20672.93     23276.12     2582.04      4553.53     
ch5      38402.76     37460.71     1937.59      2904.90     
ch6      10508.39     14849.58     4026.00      7976.12     
ch7      3333.37      5625.94      1536.68      3562.00     
ch8      30275.21     52792.89     18940.80     43501.48    
ch9      8390.77      12764.36     3386.56      6857.09     
ch10     10834.80     14376.73     3443.58      6766.22     
ch11     4973.70      7526.55      2378.34      3193.53     
ch12     29283.85     35673.68     11185.96     18944.80    
ch13     3049.77      3083.24      493.79       584.65      

‚úÖ Feature comparison saved to: enose_run_05/data/feature_comparison.csv

============================================================
COMPREHENSIVE DATA ANALYSIS
============================================================

1. FEATURE DISTRIBUTION ANALYSIS BY CLASS
--------------------------------------------------
‚úÖ Feature distributions by class saved to: enose_run_05/plots/15_feature_distributions_by_class.png

2. STATISTICAL SIGNIFICANCE ANALYSIS
--------------------------------------------------
ch0: F=244.32, p=7.01e-62, Œ∑¬≤=0.638 (High significance)
ch1: F=370.59, p=5.01e-79, Œ∑¬≤=0.728 (High significance)
ch2: F=77.75, p=1.59e-27, Œ∑¬≤=0.360 (High significance)
ch3: F=258.32, p=4.85e-64, Œ∑¬≤=0.651 (High significance)
ch4: F=174.91, p=7.58e-50, Œ∑¬≤=0.558 (High significance)
ch5: F=5.61, p=4.08e-03, Œ∑¬≤=0.039 (Medium significance)
ch6: F=427.45, p=2.14e-85, Œ∑¬≤=0.755 (High significance)
ch7: F=486.91, p=2.10e-91, Œ∑¬≤=0.779 (High significance)
ch8: F=294.13, p=3.08e-69, Œ∑¬≤=0.680 (High significance)
ch9: F=445.24, p=2.95e-87, Œ∑¬≤=0.763 (High significance)
ch10: F=272.37, p=3.91e-66, Œ∑¬≤=0.663 (High significance)
ch11: F=272.95, p=3.22e-66, Œ∑¬≤=0.663 (High significance)
ch12: F=268.21, p=1.60e-65, Œ∑¬≤=0.659 (High significance)
ch13: F=93.34, p=1.03e-31, Œ∑¬≤=0.403 (High significance)

‚úÖ Detailed statistical analysis saved to: enose_run_05/data/detailed_statistical_analysis.csv

3. OUTLIER DETECTION AND ANALYSIS
--------------------------------------------------
‚úÖ Outlier analysis plot saved to: enose_run_05/plots/16_outlier_analysis.png
Z-score outliers (>3œÉ): 20 samples affected
IQR outliers: 55 samples affected
Most problematic features (Z-score): ['ch8', 'ch9', 'ch2']

4. DATA QUALITY ASSESSMENT
--------------------------------------------------
‚úÖ Data quality metrics saved to: enose_run_05/data/data_quality_metrics.csv
Low variance features: 2
Highly skewed features (|skew| > 2): 2
  Most skewed: ['ch8', 'ch1']

5. CLASS SEPARABILITY ANALYSIS
--------------------------------------------------
‚úÖ Class separability analysis saved to: enose_run_05/plots/17_class_separability.png
‚úÖ Class separability results saved to: enose_run_05/data/class_separability.csv
Most separable features: ['ch0', 'ch6', 'ch9']
Least separable features: ['ch8', 'ch2', 'ch5']

6. DOMAIN SHIFT ANALYSIS (TRAIN VS TEST)
--------------------------------------------------
ch0: KS=0.404 (p=6.39e-02), Cohen's d=-0.903 (Low risk)
ch1: KS=0.421 (p=4.67e-02), Cohen's d=-1.162 (Medium risk)
ch2: KS=0.254 (p=4.93e-01), Cohen's d=0.179 (Low risk)
ch3: KS=0.432 (p=3.84e-02), Cohen's d=-0.853 (Medium risk)
ch4: KS=0.471 (p=1.79e-02), Cohen's d=-0.977 (Medium risk)
ch5: KS=0.271 (p=4.08e-01), Cohen's d=0.477 (Low risk)
ch6: KS=0.429 (p=4.10e-02), Cohen's d=-1.032 (Medium risk)
ch7: KS=0.436 (p=3.59e-02), Cohen's d=-1.399 (Medium risk)
ch8: KS=0.400 (p=6.79e-02), Cohen's d=-1.117 (Low risk)
ch9: KS=0.439 (p=3.36e-02), Cohen's d=-1.233 (Medium risk)
ch10: KS=0.393 (p=7.65e-02), Cohen's d=-0.985 (Low risk)
ch11: KS=0.457 (p=2.38e-02), Cohen's d=-1.060 (Medium risk)
ch12: KS=0.364 (p=1.20e-01), Cohen's d=-0.555 (Low risk)
ch13: KS=0.214 (p=6.98e-01), Cohen's d=-0.067 (Low risk)

‚úÖ Domain shift analysis saved to: enose_run_05/data/domain_shift_analysis.csv

7. FEATURE CORRELATION IMPACT ANALYSIS
--------------------------------------------------
ch0: Pearson=-0.154, Spearman=-0.243
ch1: Pearson=-0.008, Spearman=-0.158
ch2: Pearson=-0.123, Spearman=-0.134
ch3: Pearson=-0.225, Spearman=-0.242
ch4: Pearson=-0.246, Spearman=-0.213
ch5: Pearson=-0.196, Spearman=-0.192
ch6: Pearson=-0.134, Spearman=-0.236
ch7: Pearson=-0.005, Spearman=-0.108
ch8: Pearson=0.010, Spearman=-0.118
ch9: Pearson=-0.062, Spearman=-0.167
ch10: Pearson=-0.058, Spearman=-0.115
ch11: Pearson=-0.175, Spearman=-0.246
ch12: Pearson=-0.182, Spearman=-0.244
ch13: Pearson=-0.293, Spearman=-0.242

‚úÖ Target correlation analysis saved to: enose_run_05/data/target_correlation_analysis.csv

==================================================
DATA PREPARATION
==================================================
Training set size: (196, 14)
Validation set size: (84, 14)
Unclassified samples to predict: (10, 14)

==================================================
MODEL TRAINING AND EVALUATION
==================================================

Training 5 classical ML models...
------------------------------------------------------------

Training Random Forest...
‚úÖ Validation Accuracy: 0.8452

Training Support Vector Machine...
‚úÖ Validation Accuracy: 0.8333

Training K-Nearest Neighbors...
‚úÖ Validation Accuracy: 0.7976

Training Neural Network (sklearn-MLP)...
‚úÖ Validation Accuracy: 0.9524

Training Naive Bayes...
‚úÖ Validation Accuracy: 0.6786

==================================================
ENHANCED DEEP LEARNING MODEL TRAINING
==================================================

============================================================
Training Enhanced Deep MLP...
============================================================
Training on device: cuda
Epoch [10/150], Train Loss: 0.6388, Val Loss: 0.4839, Val Acc: 0.7738
Epoch [20/150], Train Loss: 0.4522, Val Loss: 0.3460, Val Acc: 0.7738
Early stopping triggered. Best Val Acc: 0.8095
Early stopping at epoch 22

‚úÖ Best Validation Accuracy: 0.8095
Final Metrics: Acc=0.7738, F1=0.7729, MCC=0.6078

============================================================
Training Enhanced Conv1D Net...
============================================================
Training on device: cuda
Epoch [10/150], Train Loss: 0.7868, Val Loss: 0.5997, Val Acc: 0.8333
Epoch [20/150], Train Loss: 0.4305, Val Loss: 0.4194, Val Acc: 0.8810
Epoch [30/150], Train Loss: 0.3384, Val Loss: 0.4674, Val Acc: 0.8690
Early stopping triggered. Best Val Acc: 0.8810
Early stopping at epoch 31

‚úÖ Best Validation Accuracy: 0.8810
Final Metrics: Acc=0.8690, F1=0.8659, MCC=0.7703

============================================================
Training Enhanced LSTM Net...
============================================================
Training on device: cuda
Epoch [10/150], Train Loss: 0.4455, Val Loss: 0.4449, Val Acc: 0.7857
Epoch [20/150], Train Loss: 0.2811, Val Loss: 0.2763, Val Acc: 0.8810
Epoch [30/150], Train Loss: 0.1009, Val Loss: 0.1473, Val Acc: 0.9405
Early stopping triggered. Best Val Acc: 0.9524
Early stopping at epoch 32

‚úÖ Best Validation Accuracy: 0.9524
Final Metrics: Acc=0.9405, F1=0.9404, MCC=0.8955

============================================================
Training Transformer Net...
============================================================
Training on device: cuda
Epoch [10/150], Train Loss: 0.4960, Val Loss: 0.3796, Val Acc: 0.8095
Early stopping triggered. Best Val Acc: 0.8214
Early stopping at epoch 14

‚úÖ Best Validation Accuracy: 0.8214
Final Metrics: Acc=0.8214, F1=0.8207, MCC=0.6905

==================================================
CREATING CONFUSION MATRICES
==================================================
‚úÖ Confusion matrices (top 6 models) saved to: enose_run_05/plots/08_confusion_matrices.png

‚úÖ Model performance results saved to: enose_run_05/data/model_performance.csv

======================================================================
MODEL PERFORMANCE SUMMARY (Sorted by Accuracy)
======================================================================
                       model    model_type  validation_accuracy  validation_precision  validation_recall  validation_specificity  validation_f1  validation_mcc  cv_mean   cv_std
Neural Network (sklearn-MLP) Deep Learning             0.952381              0.957265           0.952381                0.983730       0.952634        0.921979 0.969359 0.010323
           Enhanced LSTM Net Deep Learning             0.940476              0.940687           0.940476                0.945635       0.940403        0.895450 0.952381 0.000000
         Enhanced Conv1D Net Deep Learning             0.869048              0.871024           0.869048                0.875397       0.865914        0.770329 0.880952 0.000000
               Random Forest  Classical ML             0.845238              0.856111           0.845238                0.815873       0.839610        0.727307 0.877436 0.030269
      Support Vector Machine  Classical ML             0.833333              0.854212           0.833333                0.788889       0.819329        0.715394 0.801026 0.024997
             Transformer Net Deep Learning             0.821429              0.820571           0.821429                0.853571       0.820727        0.690461 0.821429 0.000000
         K-Nearest Neighbors  Classical ML             0.797619              0.807264           0.797619                0.763492       0.786335        0.639789 0.857051 0.041917
           Enhanced Deep MLP Deep Learning             0.773810              0.772446           0.773810                0.812302       0.772857        0.607842 0.809524 0.000000
                 Naive Bayes  Classical ML             0.678571              0.723932           0.678571                0.796429       0.684590        0.497655 0.699103 0.028332

==================================================
HYPERPARAMETER TUNING
==================================================

Tuning Random Forest...
Best parameters: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}
Best CV score: 0.7819

Tuning Support Vector Machine...
Best parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}
Best CV score: 0.7603

Tuning K-Nearest Neighbors...
Best parameters: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}
Best CV score: 0.7711

Tuning Neural Network...
Best parameters: {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}
Best CV score: 0.8320

Adding Naive Bayes with default parameters...

‚úÖ Hyperparameter tuning results saved to: enose_run_05/data/hyperparameter_tuning.csv

============================================================
COMPREHENSIVE FEATURE IMPORTANCE ANALYSIS
============================================================

1. BUILT-IN FEATURE IMPORTANCE
----------------------------------------
Random Forest: Built-in importance calculated
  Computing permutation importance for Support Vector Machine...
Support Vector Machine: Permutation importance calculated
  Computing permutation importance for K-Nearest Neighbors...
K-Nearest Neighbors: Permutation importance calculated
  Computing permutation importance for Neural Network (sklearn-MLP)...
Neural Network (sklearn-MLP): Permutation importance calculated
  Computing permutation importance for Naive Bayes...
Naive Bayes: Permutation importance calculated
  Computing permutation importance for Enhanced Deep MLP...
‚ö†Ô∏è Skipping feature importance: PyTorch model not compatible with permutation_importance
Enhanced Deep MLP: Permutation importance calculated
  Computing permutation importance for Enhanced Conv1D Net...
‚ö†Ô∏è Skipping feature importance: PyTorch model not compatible with permutation_importance
Enhanced Conv1D Net: Permutation importance calculated
  Computing permutation importance for Enhanced LSTM Net...
‚ö†Ô∏è Skipping feature importance: PyTorch model not compatible with permutation_importance
Enhanced LSTM Net: Permutation importance calculated
  Computing permutation importance for Transformer Net...
‚ö†Ô∏è Skipping feature importance: PyTorch model not compatible with permutation_importance
Transformer Net: Permutation importance calculated
Random Forest (Tuned): Built-in importance calculated
  Computing permutation importance for Support Vector Machine (Tuned)...
Support Vector Machine (Tuned): Permutation importance calculated
  Computing permutation importance for K-Nearest Neighbors (Tuned)...
K-Nearest Neighbors (Tuned): Permutation importance calculated
  Computing permutation importance for Neural Network (Tuned)...
Neural Network (Tuned): Permutation importance calculated
  Computing permutation importance for Naive Bayes (Tuned)...
Naive Bayes (Tuned): Permutation importance calculated

2. STATISTICAL FEATURE SELECTION
----------------------------------------
Statistical feature selection methods calculated

3. CREATING FEATURE IMPORTANCE VISUALIZATIONS
----------------------------------------
‚úÖ Feature importance heatmap saved to: enose_run_05/plots/18_feature_importance_heatmap.png
‚úÖ Individual feature importance plots saved to: enose_run_05/plots/19_individual_feature_importance.png
‚úÖ Feature ranking comparison saved to: enose_run_05/plots/20_feature_ranking_comparison.png

4. CONSENSUS FEATURE IMPORTANCE ANALYSIS
----------------------------------------
‚úÖ Consensus feature importance saved to: enose_run_05/plots/21_consensus_feature_importance.png

‚úÖ Comprehensive feature importance saved to: enose_run_05/data/comprehensive_feature_importance.csv

5. FEATURE IMPORTANCE SUMMARY
----------------------------------------
TOP 5 MOST CONSISTENTLY IMPORTANT FEATURES:
1. ch6: Average rank 5.5
   Scores across methods:
     Random Forest: 0.1177
     Support Vector Machine: 0.0529
     K-Nearest Neighbors: 0.0418
     Neural Network (sklearn-MLP): 0.1289
     Naive Bayes: 0.0086

2. ch0: Average rank 5.6
   Scores across methods:
     Random Forest: 0.1091
     Support Vector Machine: 0.0489
     K-Nearest Neighbors: 0.0371
     Neural Network (sklearn-MLP): 0.2164
     Naive Bayes: 0.0057

3. ch3: Average rank 5.7
   Scores across methods:
     Random Forest: 0.0582
     Support Vector Machine: 0.0504
     K-Nearest Neighbors: 0.0329
     Neural Network (sklearn-MLP): 0.2932
     Naive Bayes: 0.0032

4. ch2: Average rank 6.2
   Scores across methods:
     Random Forest: 0.0806
     Support Vector Machine: 0.0679
     K-Nearest Neighbors: 0.1304
     Neural Network (sklearn-MLP): 0.2107
     Naive Bayes: 0.0061

5. ch8: Average rank 6.7
   Scores across methods:
     Random Forest: 0.0797
     Support Vector Machine: 0.0193
     K-Nearest Neighbors: 0.0175
     Neural Network (sklearn-MLP): 0.1796
     Naive Bayes: -0.0025


============================================================
CLASS PROFILE CORRELATION & CONFUSION ANALYSIS
============================================================

1. CALCULATING CLASS SENSOR PROFILES
----------------------------------------
Mean sensor values by class:
                ch0           ch1           ch2           ch3           ch4  ...           ch9          ch10         ch11          ch12         ch13
class                                                                        ...                                                                    
ADB    11721.332456   5489.398621  31434.848109  27492.789073  20636.270547  ...   7666.166365  10146.729843  4736.240952  28249.257743  3082.984398
UFB    21338.966778  17506.437169  49419.285942  37296.766392  24887.725502  ...  15485.155468  17560.732876  9432.844508  50116.173569  3677.237962
WFB     8842.124972   4060.003375  25828.325712  23562.168102  18638.844320  ...   6292.788379   8847.990784  3219.028636  20936.871025  2669.622807

[3 rows x 14 columns]

Standard deviations by class:
               ch0          ch1           ch2          ch3          ch4          ch5  ...           ch8          ch9         ch10         ch11         ch12        ch13
class                                                                                 ...                                                                              
ADB    3552.479906  2237.967292   8314.236246  3241.848832  1692.212712  1718.794060  ...   7213.875701  1693.303841  1844.711371  1494.759625  6668.671215  390.754059
UFB    1957.657494  5339.030763  15383.792794  2136.198040  1176.279523  2599.907772  ...  22740.966790  2224.732779  3064.276187  1467.746411  7366.669211  245.917771
WFB    1845.512703  1296.019085   9304.729430  3306.149809  1998.962173  1872.172665  ...   6857.297257  1195.929437  1637.984431  1090.003008  5892.917738  423.225014

[3 rows x 14 columns]

‚úÖ Detailed class profiles saved to: enose_run_05/data/detailed_class_profiles.csv

2. INTER-CLASS CORRELATION ANALYSIS
----------------------------------------
Correlation matrix between class sensor profiles:
class       ADB       UFB       WFB
class                              
ADB    1.000000  0.836234  0.988721
UFB    0.836234  1.000000  0.774736
WFB    0.988721  0.774736  1.000000
‚úÖ Class correlation matrix saved to: enose_run_05/plots/22_class_correlation_matrix.png

3. CLASS SIMILARITY ANALYSIS
----------------------------------------

Euclidean Similarity Scores:
  ADB vs UFB: 0.157
  ADB vs WFB: 0.821
  UFB vs WFB: 0.000

Manhattan Similarity Scores:
  ADB vs UFB: 0.187
  ADB vs WFB: 0.809
  UFB vs WFB: 0.000

Cosine Similarity Scores:
  ADB vs UFB: 0.947
  ADB vs WFB: 0.995
  UFB vs WFB: 0.922

Correlation Similarity Scores:
  ADB vs UFB: 0.836
  ADB vs WFB: 0.989
  UFB vs WFB: 0.775

‚úÖ Class similarity matrices saved to: enose_run_05/plots/23_class_similarity_matrices.png

4. SENSOR-WISE CLASS DISCRIMINATION
----------------------------------------
ch0: CV=0.383, Range Ratio=0.586, Rel Std=0.383
ch1: CV=0.669, Range Ratio=0.768, Rel Std=0.669
ch2: CV=0.283, Range Ratio=0.477, Rel Std=0.283
ch3: CV=0.196, Range Ratio=0.368, Rel Std=0.196
ch4: CV=0.122, Range Ratio=0.251, Rel Std=0.122
ch5: CV=0.009, Range Ratio=0.023, Rel Std=0.009
ch6: CV=0.396, Range Ratio=0.593, Rel Std=0.396
ch7: CV=0.461, Range Ratio=0.623, Rel Std=0.461
ch8: CV=0.557, Range Ratio=0.691, Rel Std=0.557
ch9: CV=0.413, Range Ratio=0.594, Rel Std=0.413
ch10: CV=0.315, Range Ratio=0.496, Rel Std=0.315
ch11: CV=0.456, Range Ratio=0.659, Rel Std=0.456
ch12: CV=0.375, Range Ratio=0.582, Rel Std=0.375
ch13: CV=0.132, Range Ratio=0.274, Rel Std=0.132

‚úÖ Sensor discrimination analysis saved to: enose_run_05/data/sensor_discrimination_analysis.csv

5. CREATING CLASS PROFILE RADAR CHARTS
----------------------------------------
‚úÖ Class profile radar chart saved to: enose_run_05/plots/24_class_profile_radar.png

6. TEST DATA PROJECTION ANALYSIS
----------------------------------------
‚úÖ Test sample similarities saved to: enose_run_05/plots/25_test_sample_similarities.png

‚úÖ Test sample class similarities saved to: enose_run_05/data/test_sample_class_similarities.csv

7. CORRELATION & SIMILARITY SUMMARY
----------------------------------------
Most similar class pair: ADB vs WFB
Euclidean similarity: 0.821

Most discriminative sensors (by coefficient of variation):
1. ch1: CV = 0.669
2. ch8: CV = 0.557
3. ch7: CV = 0.461
4. ch11: CV = 0.456
5. ch9: CV = 0.413

Test sample classification tendencies (Euclidean similarity):
X1: Most similar to ADB (similarity: 0.000)
X2: Most similar to ADB (similarity: 0.000)
X3: Most similar to UFB (similarity: 0.621)
X4: Most similar to UFB (similarity: 0.735)
X5: Most similar to UFB (similarity: 0.750)
X6: Most similar to WFB (similarity: 0.723)
X7: Most similar to WFB (similarity: 0.725)
X8: Most similar to WFB (similarity: 0.744)
X9: Most similar to ADB (similarity: 0.807)
X10: Most similar to ADB (similarity: 0.922)

==================================================
PREDICTING UNCLASSIFIED SAMPLES
==================================================

Random Forest:
Average Prediction Confidence: 0.8930
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 0.990)
  X4: UFB (confidence: 0.970)
  X5: UFB (confidence: 0.990)
  X6: WFB (confidence: 0.690)
  X7: WFB (confidence: 0.970)
  X8: WFB (confidence: 0.610)
  X9: ADB (confidence: 0.930)
  X10: ADB (confidence: 0.780)

Support Vector Machine:
Average Prediction Confidence: 0.8282
  X1: UFB (confidence: 0.663)
  X2: UFB (confidence: 0.788)
  X3: UFB (confidence: 0.955)
  X4: UFB (confidence: 0.948)
  X5: UFB (confidence: 0.936)
  X6: WFB (confidence: 0.828)
  X7: WFB (confidence: 0.924)
  X8: WFB (confidence: 0.755)
  X9: ADB (confidence: 0.706)
  X10: ADB (confidence: 0.779)

K-Nearest Neighbors:
Average Prediction Confidence: 0.9000
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 1.000)
  X4: UFB (confidence: 0.600)
  X5: UFB (confidence: 0.800)
  X6: WFB (confidence: 0.800)
  X7: WFB (confidence: 1.000)
  X8: WFB (confidence: 0.800)
  X9: ADB (confidence: 1.000)
  X10: ADB (confidence: 1.000)

Neural Network (sklearn-MLP):
Average Prediction Confidence: 0.9322
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 0.999)
  X4: UFB (confidence: 0.990)
  X5: UFB (confidence: 0.907)
  X6: WFB (confidence: 0.999)
  X7: WFB (confidence: 1.000)
  X8: WFB (confidence: 1.000)
  X9: UFB (confidence: 0.832)
  X10: ADB (confidence: 0.595)

Naive Bayes:
Average Prediction Confidence: 0.9993
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 1.000)
  X4: UFB (confidence: 1.000)
  X5: UFB (confidence: 1.000)
  X6: WFB (confidence: 1.000)
  X7: WFB (confidence: 1.000)
  X8: WFB (confidence: 1.000)
  X9: ADB (confidence: 1.000)
  X10: ADB (confidence: 0.993)

Enhanced Deep MLP:
Average Prediction Confidence: 0.9435
  X1: 1 (confidence: 0.965)
  X2: 1 (confidence: 0.963)
  X3: 1 (confidence: 0.954)
  X4: 1 (confidence: 0.944)
  X5: 1 (confidence: 0.931)
  X6: 2 (confidence: 0.926)
  X7: 2 (confidence: 0.924)
  X8: 2 (confidence: 0.923)
  X9: 1 (confidence: 0.954)
  X10: 0 (confidence: 0.951)

Enhanced Conv1D Net:
Average Prediction Confidence: 0.9673
  X1: 1 (confidence: 1.000)
  X2: 1 (confidence: 1.000)
  X3: 1 (confidence: 1.000)
  X4: 1 (confidence: 0.989)
  X5: 1 (confidence: 0.977)
  X6: 2 (confidence: 0.998)
  X7: 2 (confidence: 0.994)
  X8: 2 (confidence: 0.984)
  X9: 0 (confidence: 0.767)
  X10: 0 (confidence: 0.964)

Enhanced LSTM Net:
Average Prediction Confidence: 0.9809
  X1: 1 (confidence: 0.998)
  X2: 1 (confidence: 0.998)
  X3: 1 (confidence: 0.998)
  X4: 1 (confidence: 0.996)
  X5: 1 (confidence: 0.986)
  X6: 2 (confidence: 0.993)
  X7: 2 (confidence: 0.998)
  X8: 2 (confidence: 0.951)
  X9: 1 (confidence: 0.892)
  X10: 0 (confidence: 0.999)

Transformer Net:
Average Prediction Confidence: 0.9201
  X1: 1 (confidence: 0.984)
  X2: 1 (confidence: 0.984)
  X3: 1 (confidence: 0.975)
  X4: 1 (confidence: 0.970)
  X5: 1 (confidence: 0.965)
  X6: 2 (confidence: 0.803)
  X7: 2 (confidence: 0.870)
  X8: 2 (confidence: 0.804)
  X9: 1 (confidence: 0.951)
  X10: 0 (confidence: 0.896)

Random Forest (Tuned):
Average Prediction Confidence: 0.8915
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 0.980)
  X4: UFB (confidence: 0.970)
  X5: UFB (confidence: 0.980)
  X6: WFB (confidence: 0.740)
  X7: WFB (confidence: 0.940)
  X8: WFB (confidence: 0.727)
  X9: ADB (confidence: 0.940)
  X10: ADB (confidence: 0.638)

Support Vector Machine (Tuned):
Average Prediction Confidence: 0.8534
  X1: UFB (confidence: 0.674)
  X2: UFB (confidence: 0.765)
  X3: UFB (confidence: 0.975)
  X4: UFB (confidence: 0.831)
  X5: UFB (confidence: 0.827)
  X6: WFB (confidence: 0.988)
  X7: WFB (confidence: 0.991)
  X8: WFB (confidence: 0.977)
  X9: ADB (confidence: 0.630)
  X10: WFB (confidence: 0.876)

K-Nearest Neighbors (Tuned):
Average Prediction Confidence: 0.9222
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 1.000)
  X4: UFB (confidence: 0.786)
  X5: UFB (confidence: 0.908)
  X6: WFB (confidence: 0.876)
  X7: WFB (confidence: 1.000)
  X8: WFB (confidence: 0.762)
  X9: ADB (confidence: 1.000)
  X10: ADB (confidence: 0.888)

Neural Network (Tuned):
Average Prediction Confidence: 0.9679
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 1.000)
  X4: UFB (confidence: 0.981)
  X5: UFB (confidence: 0.994)
  X6: WFB (confidence: 1.000)
  X7: WFB (confidence: 1.000)
  X8: WFB (confidence: 1.000)
  X9: UFB (confidence: 0.745)
  X10: ADB (confidence: 0.959)

Naive Bayes (Tuned):
Average Prediction Confidence: 0.9994
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 1.000)
  X4: UFB (confidence: 1.000)
  X5: UFB (confidence: 1.000)
  X6: WFB (confidence: 1.000)
  X7: WFB (confidence: 1.000)
  X8: WFB (confidence: 1.000)
  X9: ADB (confidence: 1.000)
  X10: ADB (confidence: 0.994)

‚úÖ All predictions saved to: enose_run_05/data/all_predictions.csv

==================================================
CREATING PREDICTION VISUALIZATIONS
==================================================
‚úÖ Model confidence plot saved to: enose_run_05/plots/09_model_confidence.png
‚úÖ Prediction distribution plot saved to: enose_run_05/plots/10_prediction_distribution.png
‚úÖ Prediction heatmap saved to: enose_run_05/plots/12_prediction_heatmap.png
‚úÖ Consensus predictions plot saved to: enose_run_05/plots/15_consensus_predictions.png
‚úÖ Confidence distribution plot saved to: enose_run_05/plots/13_confidence_dist_1_Naive_Bayes_(Tuned).png
‚úÖ Confidence distribution plot saved to: enose_run_05/plots/13_confidence_dist_2_Naive_Bayes.png
‚úÖ Confidence distribution plot saved to: enose_run_05/plots/13_confidence_dist_3_Enhanced_LSTM_Net.png
‚úÖ Per-sample confidence plot saved to: enose_run_05/plots/14_per_sample_confidence.png
‚úÖ Consensus predictions plot saved to: enose_run_05/plots/15_consensus_predictions.png

DETAILED PREDICTION SUMMARY:
==================================================

Most confident model: Naive Bayes (Tuned)
Average confidence: 0.9994

CONSENSUS PREDICTIONS:
------------------------------
X1: UFB (10/14 models)
X2: UFB (10/14 models)
X3: UFB (10/14 models)
X4: UFB (10/14 models)
X5: UFB (10/14 models)
X6: WFB (10/14 models)
X7: WFB (10/14 models)
X8: WFB (10/14 models)
X9: ADB (8/14 models)
X10: ADB (9/14 models)

‚úÖ Consensus predictions saved to: enose_run_05/data/consensus_predictions.csv

============================================================
ANALYSIS SUMMARY
============================================================

Best performing model: Naive Bayes (Tuned)
Model Class: Classical ML
Average Confidence: 0.9994

FINAL PREDICTIONS (using Naive Bayes (Tuned)):
----------------------------------------
X1: UFB (confidence: 1.000)
X2: UFB (confidence: 1.000)
X3: UFB (confidence: 1.000)
X4: UFB (confidence: 1.000)
X5: UFB (confidence: 1.000)
X6: WFB (confidence: 1.000)
X7: WFB (confidence: 1.000)
X8: WFB (confidence: 1.000)
X9: ADB (confidence: 1.000)
X10: ADB (confidence: 0.994)

PREDICTION SUMMARY:
- ADB: 2/10 samples (20.0%)
- UFB: 5/10 samples (50.0%)
- WFB: 3/10 samples (30.0%)

TOP 5 MOST CONSISTENTLY IMPORTANT SENSOR CHANNELS:
(Based on consensus across all models and statistical methods)
1. ch6: Average rank 5.5 across all methods
2. ch0: Average rank 5.6 across all methods
3. ch3: Average rank 5.7 across all methods
4. ch2: Average rank 6.2 across all methods
5. ch8: Average rank 6.7 across all methods

DATASET CHARACTERISTICS:
- Dataset type: 3_categories
- Training samples: 280
- Number of classes: 3
- Classes: ADB, UFB, WFB
- Unclassified cocoa bean samples: 10 (X1-X10)
- Sensor channels: 14 (ch0-ch13)
- Models analyzed: Random Forest, SVM, KNN, Neural Network, Naive Bayes

============================================================
FINAL CLASSIFICATION RESULTS
============================================================
Sample_ID Predicted_Class  Confidence          Model_Used  Model_Class
       X1             UFB    1.000000 Naive Bayes (Tuned) Classical ML
       X2             UFB    1.000000 Naive Bayes (Tuned) Classical ML
       X3             UFB    1.000000 Naive Bayes (Tuned) Classical ML
       X4             UFB    1.000000 Naive Bayes (Tuned) Classical ML
       X5             UFB    1.000000 Naive Bayes (Tuned) Classical ML
       X6             WFB    1.000000 Naive Bayes (Tuned) Classical ML
       X7             WFB    1.000000 Naive Bayes (Tuned) Classical ML
       X8             WFB    1.000000 Naive Bayes (Tuned) Classical ML
       X9             ADB    1.000000 Naive Bayes (Tuned) Classical ML
      X10             ADB    0.994305 Naive Bayes (Tuned) Classical ML

‚úÖ Final classification results saved to: enose_run_05/data/final_classification_results.csv

============================================================
INTERPRETATION:
============================================================
- WFB, ADB, UFB represent different fermentation levels of cocoa beans
  * WFB: Well-Fermented Beans
  * ADB: Adequately-Fermented Beans
  * UFB: Under-Fermented Beans
- X1-X10 are unclassified cocoa bean samples
- The model predicts which known category each sample belongs to
- Higher confidence scores indicate more reliable predictions
- Consider validating results with domain experts
============================================================

‚úÖ Comprehensive summary report saved to: enose_run_05/ANALYSIS_SUMMARY_REPORT_20251123_054216.txt

============================================================
ANALYSIS COMPLETE - ALL RESULTS SAVED
============================================================
üìÅ Results directory: enose_run_05
üìä Individual plots (25 files) saved in: enose_run_05/plots
üìã Data files (15 CSV files) saved in: enose_run_05/data
üìù Log files saved in: enose_run_05/logs
üìÑ Summary report: ANALYSIS_SUMMARY_REPORT_20251123_054216.txt
üî¢ Dataset type: 3_categories
============================================================

üéâ Comprehensive analysis complete with confusion matrices!
   ‚Ä¢ 5 Classical ML models trained and evaluated
   ‚Ä¢ 4 Advanced deep learning models with modern architectures
   ‚Ä¢ Comprehensive evaluation: Accuracy, Precision, Recall, Specificity, F1-Score, MCC
   ‚Ä¢ Confusion matrices visualization similar to research papers
   ‚Ä¢ 3 classes: ADB, UFB, WFB
   ‚Ä¢ Enhanced hyperparameter tuning for better model performance
   ‚Ä¢ Individual plots covering all aspects including class correlations
   ‚Ä¢ 15 CSV files with detailed results and comprehensive metrics
   Each file can be used separately in presentations or publications.

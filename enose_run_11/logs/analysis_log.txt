E-NOSE COCOA BEAN CLASSIFICATION WITH MULTIPLE ML MODELS
======================================================================
Analysis started at: 2025-10-27 12:51:33
Results will be saved to: enose_run_11
======================================================================
Loading and preprocessing data...
Raw training data shape: (280, 15)
Raw testing data shape: (10, 15)

Training categories: ['WFB' 'UFB' 'ADB']
Testing sample IDs: ['X1' 'X2' 'X3' 'X4' 'X5' 'X6' 'X7' 'X8' 'X9' 'X10']
Classes in training data: ['WFB' 'UFB' 'ADB']

=== FINAL PROCESSED DATA ===
Training data shape: (280, 15)
Testing data shape: (10, 15)
Training classes: ['WFB' 'UFB' 'ADB']
Training class distribution:
class
ADB    160
WFB     80
UFB     40
Name: count, dtype: int64
Unclassified samples to predict: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10']

==================================================
EXPLORATORY DATA ANALYSIS
==================================================

Training Data Statistics:
                ch0           ch1           ch2           ch3           ch4           ch5           ch6          ch7            ch8           ch9          ch10          ch11          ch12         ch13
count    280.000000    280.000000    280.000000    280.000000    280.000000    280.000000    280.000000   280.000000     280.000000    280.000000    280.000000    280.000000    280.000000   280.000000
mean   12272.649507   6797.719771  32402.189972  27770.322698  20672.928047  38402.758895  10508.392174  3333.367171   30275.208106   8390.771098  10834.804831   4973.695084  29283.849513  3049.774453
std     4901.600320   5185.205904  12305.955995   5277.818615   2586.662169   1941.057205   4033.206432  1539.433154   18974.712654   3392.621735   3449.741107   2382.600032  11205.990374   494.676293
min     5084.054294   2149.330500  12005.522719  16179.081820  13981.864820  29500.580074   4446.287171  1613.494835   12392.280363   4002.401331   5779.140141   1239.243108   7375.124645  1968.592680
25%     8995.110880   3863.362292  23927.217773  24072.775316  18994.512013  37171.470789   7772.327169  2424.006120   19313.010457   6254.773159   8763.570607   3308.385756  21690.109859  2732.249535
50%    10945.927803   5102.648086  31451.724005  26917.029187  20458.729005  38814.037413   9378.835235  2785.382675   23737.019001   7253.549216   9870.847421   4356.154697  26807.964112  2994.682423
75%    13815.757665   6626.304858  37760.750470  30142.617809  22106.499015  39713.070817  11629.444257  3371.921914   31729.953627   8897.995681  11509.929764   5842.586037  33021.808566  3386.057509
max    42731.830954  32783.814275  78648.664073  40549.214519  26565.280337  42161.966686  23568.203788  9417.646270  111405.407566  21740.973078  24537.035018  13982.151828  65480.362365  4401.078142

✅ Class distribution plot saved to: enose_run_11/plots/01_class_distribution.png
✅ Sample distribution plot saved to: enose_run_11/plots/02_sample_distribution.png
✅ Correlation matrix plot saved to: enose_run_11/plots/03_correlation_matrix.png
✅ Sensor boxplot saved to: enose_run_11/plots/04_sensor_boxplot.png
✅ Feature importance plot saved to: enose_run_11/plots/05_feature_importance.png
✅ PCA visualization plot saved to: enose_run_11/plots/06_pca_visualization.png
✅ Class means comparison plot saved to: enose_run_11/plots/07_class_means_comparison.png

Training Data Class-wise Feature Analysis:
========================================

Mean sensor values by class (training data):
                ch0           ch1           ch2           ch3           ch4           ch5           ch6          ch7           ch8           ch9          ch10         ch11          ch12         ch13
class                                                                                                                                                                                                 
ADB    11721.332456   5489.398621  31434.848109  27492.789073  20636.270547  38698.763647   9895.916813  2928.724766  25374.991557   7666.166365  10146.729843  4736.240952  28249.257743  3082.984398
UFB    21338.966778  17506.437169  49419.285942  37296.766392  24887.725502  38378.518027  18727.424375  6622.553466  68254.154449  15485.155468  17560.732876  9432.844508  50116.173569  3677.237962
WFB     8842.124972   4060.003375  25828.325712  23562.168102  18638.844320  37822.869824   7623.826793  2498.058832  21086.168032   6292.788379   8847.990784  3219.028636  20936.871025  2669.622807

✅ Class means saved to: enose_run_11/data/class_means.csv

Class separability analysis (ANOVA F-statistic for each feature):
ch0: F=244.32, p=7.01e-62
ch1: F=370.59, p=5.01e-79
ch2: F=77.75, p=1.59e-27
ch3: F=258.32, p=4.85e-64
ch4: F=174.91, p=7.58e-50
ch5: F=5.61, p=4.08e-03
ch6: F=427.45, p=2.14e-85
ch7: F=486.91, p=2.10e-91
ch8: F=294.13, p=3.08e-69
ch9: F=445.24, p=2.95e-87
ch10: F=272.37, p=3.91e-66
ch11: F=272.95, p=3.22e-66
ch12: F=268.21, p=1.60e-65
ch13: F=93.34, p=1.03e-31

✅ ANOVA results saved to: enose_run_11/data/anova_results.csv

Feature Distribution Comparison (Training vs Unclassified Samples):
============================================================
Feature  Train Mean   Test Mean    Train Std    Test Std    
------------------------------------------------------------
ch0      12272.65     16855.74     4892.84      9045.62     
ch1      6797.72      13148.83     5175.94      11174.04    
ch2      32402.19     30214.49     12283.96     11300.89    
ch3      27770.32     32432.08     5268.39      9803.32     
ch4      20672.93     23276.12     2582.04      4553.53     
ch5      38402.76     37460.71     1937.59      2904.90     
ch6      10508.39     14849.58     4026.00      7976.12     
ch7      3333.37      5625.94      1536.68      3562.00     
ch8      30275.21     52792.89     18940.80     43501.48    
ch9      8390.77      12764.36     3386.56      6857.09     
ch10     10834.80     14376.73     3443.58      6766.22     
ch11     4973.70      7526.55      2378.34      3193.53     
ch12     29283.85     35673.68     11185.96     18944.80    
ch13     3049.77      3083.24      493.79       584.65      

✅ Feature comparison saved to: enose_run_11/data/feature_comparison.csv

============================================================
COMPREHENSIVE DATA ANALYSIS
============================================================

1. FEATURE DISTRIBUTION ANALYSIS BY CLASS
--------------------------------------------------
✅ Feature distributions by class saved to: enose_run_11/plots/15_feature_distributions_by_class.png

2. STATISTICAL SIGNIFICANCE ANALYSIS
--------------------------------------------------
ch0: F=244.32, p=7.01e-62, η²=0.638 (High significance)
ch1: F=370.59, p=5.01e-79, η²=0.728 (High significance)
ch2: F=77.75, p=1.59e-27, η²=0.360 (High significance)
ch3: F=258.32, p=4.85e-64, η²=0.651 (High significance)
ch4: F=174.91, p=7.58e-50, η²=0.558 (High significance)
ch5: F=5.61, p=4.08e-03, η²=0.039 (Medium significance)
ch6: F=427.45, p=2.14e-85, η²=0.755 (High significance)
ch7: F=486.91, p=2.10e-91, η²=0.779 (High significance)
ch8: F=294.13, p=3.08e-69, η²=0.680 (High significance)
ch9: F=445.24, p=2.95e-87, η²=0.763 (High significance)
ch10: F=272.37, p=3.91e-66, η²=0.663 (High significance)
ch11: F=272.95, p=3.22e-66, η²=0.663 (High significance)
ch12: F=268.21, p=1.60e-65, η²=0.659 (High significance)
ch13: F=93.34, p=1.03e-31, η²=0.403 (High significance)

✅ Detailed statistical analysis saved to: enose_run_11/data/detailed_statistical_analysis.csv

3. OUTLIER DETECTION AND ANALYSIS
--------------------------------------------------
✅ Outlier analysis plot saved to: enose_run_11/plots/16_outlier_analysis.png
Z-score outliers (>3σ): 20 samples affected
IQR outliers: 55 samples affected
Most problematic features (Z-score): ['ch8', 'ch9', 'ch2']

4. DATA QUALITY ASSESSMENT
--------------------------------------------------
✅ Data quality metrics saved to: enose_run_11/data/data_quality_metrics.csv
Low variance features: 2
Highly skewed features (|skew| > 2): 2
  Most skewed: ['ch8', 'ch1']

5. CLASS SEPARABILITY ANALYSIS
--------------------------------------------------
✅ Class separability analysis saved to: enose_run_11/plots/17_class_separability.png
✅ Class separability results saved to: enose_run_11/data/class_separability.csv
Most separable features: ['ch0', 'ch6', 'ch9']
Least separable features: ['ch8', 'ch2', 'ch5']

6. DOMAIN SHIFT ANALYSIS (TRAIN VS TEST)
--------------------------------------------------
ch0: KS=0.404 (p=6.39e-02), Cohen's d=-0.903 (Low risk)
ch1: KS=0.421 (p=4.67e-02), Cohen's d=-1.162 (Medium risk)
ch2: KS=0.254 (p=4.93e-01), Cohen's d=0.179 (Low risk)
ch3: KS=0.432 (p=3.84e-02), Cohen's d=-0.853 (Medium risk)
ch4: KS=0.471 (p=1.79e-02), Cohen's d=-0.977 (Medium risk)
ch5: KS=0.271 (p=4.08e-01), Cohen's d=0.477 (Low risk)
ch6: KS=0.429 (p=4.10e-02), Cohen's d=-1.032 (Medium risk)
ch7: KS=0.436 (p=3.59e-02), Cohen's d=-1.399 (Medium risk)
ch8: KS=0.400 (p=6.79e-02), Cohen's d=-1.117 (Low risk)
ch9: KS=0.439 (p=3.36e-02), Cohen's d=-1.233 (Medium risk)
ch10: KS=0.393 (p=7.65e-02), Cohen's d=-0.985 (Low risk)
ch11: KS=0.457 (p=2.38e-02), Cohen's d=-1.060 (Medium risk)
ch12: KS=0.364 (p=1.20e-01), Cohen's d=-0.555 (Low risk)
ch13: KS=0.214 (p=6.98e-01), Cohen's d=-0.067 (Low risk)

✅ Domain shift analysis saved to: enose_run_11/data/domain_shift_analysis.csv

7. FEATURE CORRELATION IMPACT ANALYSIS
--------------------------------------------------
ch0: Pearson=-0.154, Spearman=-0.243
ch1: Pearson=-0.008, Spearman=-0.158
ch2: Pearson=-0.123, Spearman=-0.134
ch3: Pearson=-0.225, Spearman=-0.242
ch4: Pearson=-0.246, Spearman=-0.213
ch5: Pearson=-0.196, Spearman=-0.192
ch6: Pearson=-0.134, Spearman=-0.236
ch7: Pearson=-0.005, Spearman=-0.108
ch8: Pearson=0.010, Spearman=-0.118
ch9: Pearson=-0.062, Spearman=-0.167
ch10: Pearson=-0.058, Spearman=-0.115
ch11: Pearson=-0.175, Spearman=-0.246
ch12: Pearson=-0.182, Spearman=-0.244
ch13: Pearson=-0.293, Spearman=-0.242

✅ Target correlation analysis saved to: enose_run_11/data/target_correlation_analysis.csv

==================================================
DATA PREPARATION
==================================================
Training set size: (196, 14)
Validation set size: (84, 14)
Unclassified samples to predict: (10, 14)
Sample IDs: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10']

==================================================
MODEL TRAINING AND EVALUATION
==================================================
Training 5 classical ML models: Random Forest, Support Vector Machine, K-Nearest Neighbors, Neural Network (sklearn-MLP), Naive Bayes
------------------------------------------------------------

Training Random Forest...
Validation Results: {'accuracy': 0.8452380952380952, 'precision': 0.8561108653987911, 'recall': 0.8452380952380952, 'specificity': np.float64(0.8158730158730158), 'f1_score': 0.8396096354284507, 'mcc': 0.7273065072554439}

Training Support Vector Machine...
Validation Results: {'accuracy': 0.8333333333333334, 'precision': 0.8542124542124542, 'recall': 0.8333333333333334, 'specificity': np.float64(0.7888888888888889), 'f1_score': 0.819328645415602, 'mcc': 0.7153937349538297}

Training K-Nearest Neighbors...
Validation Results: {'accuracy': 0.7976190476190477, 'precision': 0.8072639225181598, 'recall': 0.7976190476190477, 'specificity': np.float64(0.7634920634920634), 'f1_score': 0.7863351601669359, 'mcc': 0.639788770087183}

Training Neural Network (sklearn-MLP)...
Validation Results: {'accuracy': 0.9523809523809523, 'precision': 0.9572649572649572, 'recall': 0.9523809523809523, 'specificity': np.float64(0.9837301587301586), 'f1_score': 0.9526342710997442, 'mcc': 0.9219792183644339}

Training Naive Bayes...
Validation Results: {'accuracy': 0.6785714285714286, 'precision': 0.723931623931624, 'recall': 0.6785714285714286, 'specificity': np.float64(0.7964285714285715), 'f1_score': 0.6845897877223178, 'mcc': 0.49765499437180905}

==================================================
DEEP LEARNING MODEL TRAINING
==================================================

Training Deep MLP...
Validation Results (Deep MLP): {'accuracy': 0.9285714285714286, 'precision': 0.9284519827998088, 'recall': 0.9285714285714286, 'specificity': np.float64(0.940873015873016), 'f1_score': 0.928176291793313, 'mcc': 0.875590675855009}

Training Conv1D Net...
Validation Results (Conv1D Net): {'accuracy': 0.9166666666666666, 'precision': 0.9188915432393694, 'recall': 0.9166666666666666, 'specificity': np.float64(0.95), 'f1_score': 0.917228459772967, 'mcc': 0.8570165068935383}

Training LSTM Net...
Validation Results (LSTM Net): {'accuracy': 0.7857142857142857, 'precision': 0.7829004329004329, 'recall': 0.7857142857142857, 'specificity': np.float64(0.805952380952381), 'f1_score': 0.7837600878860862, 'mcc': 0.6211585466400743}

==================================================
CREATING CONFUSION MATRICES
==================================================
✅ Confusion matrices saved to: enose_run_11/plots/08_confusion_matrices.png

✅ Model performance results saved to: enose_run_11/data/model_performance.csv

==================================================
HYPERPARAMETER TUNING
==================================================

Tuning Random Forest...
Best parameters: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}
Best CV score: 0.7819

Tuning Support Vector Machine...
Best parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}
Best CV score: 0.7603

Tuning K-Nearest Neighbors...
Best parameters: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}
Best CV score: 0.7711

Tuning Neural Network...
Best parameters: {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}
Best CV score: 0.8320

Adding Naive Bayes with default parameters...
Naive Bayes CV score: 0.6782

✅ Hyperparameter tuning results saved to: enose_run_11/data/hyperparameter_tuning.csv

============================================================
COMPREHENSIVE FEATURE IMPORTANCE ANALYSIS
============================================================

1. BUILT-IN FEATURE IMPORTANCE
----------------------------------------
Random Forest: Built-in importance calculated
  Computing permutation importance for Support Vector Machine...
Support Vector Machine: Permutation importance calculated
  Computing permutation importance for K-Nearest Neighbors...
K-Nearest Neighbors: Permutation importance calculated
  Computing permutation importance for Neural Network (sklearn-MLP)...
Neural Network (sklearn-MLP): Permutation importance calculated
  Computing permutation importance for Naive Bayes...
Naive Bayes: Permutation importance calculated
  Computing permutation importance for Deep MLP...
⚠️ Skipping feature importance: PyTorch model not compatible with permutation_importance
Deep MLP: Permutation importance calculated
  Computing permutation importance for Conv1D Net...
⚠️ Skipping feature importance: PyTorch model not compatible with permutation_importance
Conv1D Net: Permutation importance calculated
  Computing permutation importance for LSTM Net...
⚠️ Skipping feature importance: PyTorch model not compatible with permutation_importance
LSTM Net: Permutation importance calculated
Random Forest (Tuned): Built-in importance calculated
  Computing permutation importance for Support Vector Machine (Tuned)...
Support Vector Machine (Tuned): Permutation importance calculated
  Computing permutation importance for K-Nearest Neighbors (Tuned)...
K-Nearest Neighbors (Tuned): Permutation importance calculated
  Computing permutation importance for Neural Network (Tuned)...
Neural Network (Tuned): Permutation importance calculated
  Computing permutation importance for Naive Bayes (Tuned)...
Naive Bayes (Tuned): Permutation importance calculated

2. STATISTICAL FEATURE SELECTION
----------------------------------------
Statistical feature selection methods calculated

3. CREATING FEATURE IMPORTANCE VISUALIZATIONS
----------------------------------------
✅ Feature importance heatmap saved to: enose_run_11/plots/18_feature_importance_heatmap.png
✅ Individual feature importance plots saved to: enose_run_11/plots/19_individual_feature_importance.png
✅ Feature ranking comparison saved to: enose_run_11/plots/20_feature_ranking_comparison.png

4. CONSENSUS FEATURE IMPORTANCE ANALYSIS
----------------------------------------
✅ Consensus feature importance saved to: enose_run_11/plots/21_consensus_feature_importance.png

✅ Comprehensive feature importance saved to: enose_run_11/data/comprehensive_feature_importance.csv

5. FEATURE IMPORTANCE SUMMARY
----------------------------------------
TOP 5 MOST CONSISTENTLY IMPORTANT FEATURES:
1. ch6: Average rank 5.5
   Scores across methods:
     Random Forest: 0.1177
     Support Vector Machine: 0.0529
     K-Nearest Neighbors: 0.0418
     Neural Network (sklearn-MLP): 0.1289
     Naive Bayes: 0.0086

2. ch0: Average rank 5.6
   Scores across methods:
     Random Forest: 0.1091
     Support Vector Machine: 0.0489
     K-Nearest Neighbors: 0.0371
     Neural Network (sklearn-MLP): 0.2164
     Naive Bayes: 0.0057

3. ch3: Average rank 5.7
   Scores across methods:
     Random Forest: 0.0582
     Support Vector Machine: 0.0504
     K-Nearest Neighbors: 0.0329
     Neural Network (sklearn-MLP): 0.2932
     Naive Bayes: 0.0032

4. ch2: Average rank 6.2
   Scores across methods:
     Random Forest: 0.0806
     Support Vector Machine: 0.0679
     K-Nearest Neighbors: 0.1304
     Neural Network (sklearn-MLP): 0.2107
     Naive Bayes: 0.0061

5. ch8: Average rank 6.7
   Scores across methods:
     Random Forest: 0.0797
     Support Vector Machine: 0.0193
     K-Nearest Neighbors: 0.0175
     Neural Network (sklearn-MLP): 0.1796
     Naive Bayes: -0.0025


============================================================
CLASS PROFILE CORRELATION & CONFUSION ANALYSIS
============================================================

1. CALCULATING CLASS SENSOR PROFILES
----------------------------------------
Mean sensor values by class:
                ch0           ch1           ch2           ch3           ch4           ch5           ch6          ch7           ch8           ch9          ch10         ch11          ch12         ch13
class                                                                                                                                                                                                 
ADB    11721.332456   5489.398621  31434.848109  27492.789073  20636.270547  38698.763647   9895.916813  2928.724766  25374.991557   7666.166365  10146.729843  4736.240952  28249.257743  3082.984398
UFB    21338.966778  17506.437169  49419.285942  37296.766392  24887.725502  38378.518027  18727.424375  6622.553466  68254.154449  15485.155468  17560.732876  9432.844508  50116.173569  3677.237962
WFB     8842.124972   4060.003375  25828.325712  23562.168102  18638.844320  37822.869824   7623.826793  2498.058832  21086.168032   6292.788379   8847.990784  3219.028636  20936.871025  2669.622807

Standard deviations by class:
               ch0          ch1           ch2          ch3          ch4          ch5          ch6          ch7           ch8          ch9         ch10         ch11         ch12        ch13
class                                                                                                                                                                                       
ADB    3552.479906  2237.967292   8314.236246  3241.848832  1692.212712  1718.794060  2197.351265   692.109184   7213.875701  1693.303841  1844.711371  1494.759625  6668.671215  390.754059
UFB    1957.657494  5339.030763  15383.792794  2136.198040  1176.279523  2599.907772  1942.386786  1117.242895  22740.966790  2224.732779  3064.276187  1467.746411  7366.669211  245.917771
WFB    1845.512703  1296.019085   9304.729430  3306.149809  1998.962173  1872.172665  1574.308652   522.629788   6857.297257  1195.929437  1637.984431  1090.003008  5892.917738  423.225014

✅ Detailed class profiles saved to: enose_run_11/data/detailed_class_profiles.csv

2. INTER-CLASS CORRELATION ANALYSIS
----------------------------------------
Correlation matrix between class sensor profiles:
class       ADB       UFB       WFB
class                              
ADB    1.000000  0.836234  0.988721
UFB    0.836234  1.000000  0.774736
WFB    0.988721  0.774736  1.000000
✅ Class correlation matrix saved to: enose_run_11/plots/22_class_correlation_matrix.png

3. CLASS SIMILARITY ANALYSIS
----------------------------------------

Euclidean Similarity Scores:
  ADB vs UFB: 0.157
  ADB vs WFB: 0.821
  UFB vs WFB: 0.000

Manhattan Similarity Scores:
  ADB vs UFB: 0.187
  ADB vs WFB: 0.809
  UFB vs WFB: 0.000

Cosine Similarity Scores:
  ADB vs UFB: 0.947
  ADB vs WFB: 0.995
  UFB vs WFB: 0.922

Correlation Similarity Scores:
  ADB vs UFB: 0.836
  ADB vs WFB: 0.989
  UFB vs WFB: 0.775

✅ Class similarity matrices saved to: enose_run_11/plots/23_class_similarity_matrices.png

4. SENSOR-WISE CLASS DISCRIMINATION
----------------------------------------
ch0: CV=0.383, Range Ratio=0.586, Rel Std=0.383
ch1: CV=0.669, Range Ratio=0.768, Rel Std=0.669
ch2: CV=0.283, Range Ratio=0.477, Rel Std=0.283
ch3: CV=0.196, Range Ratio=0.368, Rel Std=0.196
ch4: CV=0.122, Range Ratio=0.251, Rel Std=0.122
ch5: CV=0.009, Range Ratio=0.023, Rel Std=0.009
ch6: CV=0.396, Range Ratio=0.593, Rel Std=0.396
ch7: CV=0.461, Range Ratio=0.623, Rel Std=0.461
ch8: CV=0.557, Range Ratio=0.691, Rel Std=0.557
ch9: CV=0.413, Range Ratio=0.594, Rel Std=0.413
ch10: CV=0.315, Range Ratio=0.496, Rel Std=0.315
ch11: CV=0.456, Range Ratio=0.659, Rel Std=0.456
ch12: CV=0.375, Range Ratio=0.582, Rel Std=0.375
ch13: CV=0.132, Range Ratio=0.274, Rel Std=0.132

✅ Sensor discrimination analysis saved to: enose_run_11/data/sensor_discrimination_analysis.csv

5. CREATING CLASS PROFILE RADAR CHARTS
----------------------------------------
✅ Class profile radar chart saved to: enose_run_11/plots/24_class_profile_radar.png

6. TEST DATA PROJECTION ANALYSIS
----------------------------------------
✅ Test sample similarities saved to: enose_run_11/plots/25_test_sample_similarities.png

✅ Test sample class similarities saved to: enose_run_11/data/test_sample_class_similarities.csv

7. CORRELATION & SIMILARITY SUMMARY
----------------------------------------
Most similar class pair: ADB vs WFB
Euclidean similarity: 0.821

Most discriminative sensors (by coefficient of variation):
1. ch1: CV = 0.669
2. ch8: CV = 0.557
3. ch7: CV = 0.461
4. ch11: CV = 0.456
5. ch9: CV = 0.413

Test sample classification tendencies (Euclidean similarity):
X1: Most similar to ADB (similarity: 0.000)
X2: Most similar to ADB (similarity: 0.000)
X3: Most similar to UFB (similarity: 0.621)
X4: Most similar to UFB (similarity: 0.735)
X5: Most similar to UFB (similarity: 0.750)
X6: Most similar to WFB (similarity: 0.723)
X7: Most similar to WFB (similarity: 0.725)
X8: Most similar to WFB (similarity: 0.744)
X9: Most similar to ADB (similarity: 0.807)
X10: Most similar to ADB (similarity: 0.922)

==================================================
PREDICTING UNCLASSIFIED SAMPLES
==================================================

Random Forest:
Average Prediction Confidence: 0.8930
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 0.990)
  X4: UFB (confidence: 0.970)
  X5: UFB (confidence: 0.990)
  X6: WFB (confidence: 0.690)
  X7: WFB (confidence: 0.970)
  X8: WFB (confidence: 0.610)
  X9: ADB (confidence: 0.930)
  X10: ADB (confidence: 0.780)

Support Vector Machine:
Average Prediction Confidence: 0.8282
  X1: UFB (confidence: 0.663)
  X2: UFB (confidence: 0.788)
  X3: UFB (confidence: 0.955)
  X4: UFB (confidence: 0.948)
  X5: UFB (confidence: 0.936)
  X6: WFB (confidence: 0.828)
  X7: WFB (confidence: 0.924)
  X8: WFB (confidence: 0.755)
  X9: ADB (confidence: 0.706)
  X10: ADB (confidence: 0.779)

K-Nearest Neighbors:
Average Prediction Confidence: 0.9000
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 1.000)
  X4: UFB (confidence: 0.600)
  X5: UFB (confidence: 0.800)
  X6: WFB (confidence: 0.800)
  X7: WFB (confidence: 1.000)
  X8: WFB (confidence: 0.800)
  X9: ADB (confidence: 1.000)
  X10: ADB (confidence: 1.000)

Neural Network (sklearn-MLP):
Average Prediction Confidence: 0.9322
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 0.999)
  X4: UFB (confidence: 0.990)
  X5: UFB (confidence: 0.907)
  X6: WFB (confidence: 0.999)
  X7: WFB (confidence: 1.000)
  X8: WFB (confidence: 1.000)
  X9: UFB (confidence: 0.832)
  X10: ADB (confidence: 0.595)

Naive Bayes:
Average Prediction Confidence: 0.9993
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 1.000)
  X4: UFB (confidence: 1.000)
  X5: UFB (confidence: 1.000)
  X6: WFB (confidence: 1.000)
  X7: WFB (confidence: 1.000)
  X8: WFB (confidence: 1.000)
  X9: ADB (confidence: 1.000)
  X10: ADB (confidence: 0.993)

Deep MLP:
Average Prediction Confidence: 0.8283
  X1: 1 (confidence: 0.941)
  X2: 1 (confidence: 0.959)
  X3: 1 (confidence: 0.971)
  X4: 1 (confidence: 0.732)
  X5: 1 (confidence: 0.722)
  X6: 2 (confidence: 0.931)
  X7: 2 (confidence: 0.978)
  X8: 2 (confidence: 0.958)
  X9: 1 (confidence: 0.479)
  X10: 2 (confidence: 0.613)

Conv1D Net:
Average Prediction Confidence: 0.9732
  X1: 1 (confidence: 1.000)
  X2: 1 (confidence: 1.000)
  X3: 1 (confidence: 1.000)
  X4: 1 (confidence: 0.999)
  X5: 1 (confidence: 0.980)
  X6: 2 (confidence: 1.000)
  X7: 2 (confidence: 1.000)
  X8: 2 (confidence: 0.997)
  X9: 1 (confidence: 0.767)
  X10: 0 (confidence: 0.990)

LSTM Net:
Average Prediction Confidence: 0.8800
  X1: 1 (confidence: 0.999)
  X2: 1 (confidence: 0.999)
  X3: 1 (confidence: 0.933)
  X4: 1 (confidence: 0.923)
  X5: 1 (confidence: 0.855)
  X6: 2 (confidence: 0.840)
  X7: 2 (confidence: 0.907)
  X8: 2 (confidence: 0.832)
  X9: 0 (confidence: 0.694)
  X10: 0 (confidence: 0.819)

Random Forest (Tuned):
Average Prediction Confidence: 0.8915
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 0.980)
  X4: UFB (confidence: 0.970)
  X5: UFB (confidence: 0.980)
  X6: WFB (confidence: 0.740)
  X7: WFB (confidence: 0.940)
  X8: WFB (confidence: 0.727)
  X9: ADB (confidence: 0.940)
  X10: ADB (confidence: 0.638)

Support Vector Machine (Tuned):
Average Prediction Confidence: 0.8534
  X1: UFB (confidence: 0.674)
  X2: UFB (confidence: 0.765)
  X3: UFB (confidence: 0.975)
  X4: UFB (confidence: 0.831)
  X5: UFB (confidence: 0.827)
  X6: WFB (confidence: 0.988)
  X7: WFB (confidence: 0.991)
  X8: WFB (confidence: 0.977)
  X9: ADB (confidence: 0.630)
  X10: WFB (confidence: 0.876)

K-Nearest Neighbors (Tuned):
Average Prediction Confidence: 0.9222
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 1.000)
  X4: UFB (confidence: 0.786)
  X5: UFB (confidence: 0.908)
  X6: WFB (confidence: 0.876)
  X7: WFB (confidence: 1.000)
  X8: WFB (confidence: 0.762)
  X9: ADB (confidence: 1.000)
  X10: ADB (confidence: 0.888)

Neural Network (Tuned):
Average Prediction Confidence: 0.9679
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 1.000)
  X4: UFB (confidence: 0.981)
  X5: UFB (confidence: 0.994)
  X6: WFB (confidence: 1.000)
  X7: WFB (confidence: 1.000)
  X8: WFB (confidence: 1.000)
  X9: UFB (confidence: 0.745)
  X10: ADB (confidence: 0.959)

Naive Bayes (Tuned):
Average Prediction Confidence: 0.9994
  X1: UFB (confidence: 1.000)
  X2: UFB (confidence: 1.000)
  X3: UFB (confidence: 1.000)
  X4: UFB (confidence: 1.000)
  X5: UFB (confidence: 1.000)
  X6: WFB (confidence: 1.000)
  X7: WFB (confidence: 1.000)
  X8: WFB (confidence: 1.000)
  X9: ADB (confidence: 1.000)
  X10: ADB (confidence: 0.994)

✅ All predictions saved to: enose_run_11/data/all_predictions.csv

==================================================
CREATING PREDICTION VISUALIZATIONS
==================================================
✅ Model confidence plot saved to: enose_run_11/plots/09_model_confidence.png
✅ Prediction distribution plot saved to: enose_run_11/plots/10_prediction_distribution.png
✅ Model agreement plot saved to: enose_run_11/plots/11_model_agreement.png
✅ Prediction heatmap saved to: enose_run_11/plots/12_prediction_heatmap.png
✅ Confidence distribution plot saved to: enose_run_11/plots/13_confidence_dist_1_Naive_Bayes_(Tuned).png
✅ Confidence distribution plot saved to: enose_run_11/plots/13_confidence_dist_2_Naive_Bayes.png
✅ Confidence distribution plot saved to: enose_run_11/plots/13_confidence_dist_3_Conv1D_Net.png
✅ Per-sample confidence plot saved to: enose_run_11/plots/14_per_sample_confidence.png
✅ Consensus predictions plot saved to: enose_run_11/plots/15_consensus_predictions.png

DETAILED PREDICTION SUMMARY:
==================================================

Most confident model: Naive Bayes (Tuned)
Average confidence: 0.9994

CONSENSUS PREDICTIONS:
------------------------------
X1: UFB (10/13 models)
X2: UFB (10/13 models)
X3: UFB (10/13 models)
X4: UFB (10/13 models)
X5: UFB (10/13 models)
X6: WFB (10/13 models)
X7: WFB (10/13 models)
X8: WFB (10/13 models)
X9: ADB (8/13 models)
X10: ADB (9/13 models)

✅ Consensus predictions saved to: enose_run_11/data/consensus_predictions.csv

============================================================
ANALYSIS SUMMARY
============================================================

Most confident model: Naive Bayes (Tuned)
Average Prediction Confidence: 0.9994304553522106

FINAL PREDICTIONS (using Naive Bayes (Tuned)):
----------------------------------------
X1: UFB (confidence: 1.000)
X2: UFB (confidence: 1.000)
X3: UFB (confidence: 1.000)
X4: UFB (confidence: 1.000)
X5: UFB (confidence: 1.000)
X6: WFB (confidence: 1.000)
X7: WFB (confidence: 1.000)
X8: WFB (confidence: 1.000)
X9: ADB (confidence: 1.000)
X10: ADB (confidence: 0.994)

PREDICTION SUMMARY:
- ADB: 2/10 samples (20.0%)
- UFB: 5/10 samples (50.0%)
- WFB: 3/10 samples (30.0%)

TOP 5 MOST CONSISTENTLY IMPORTANT SENSOR CHANNELS:
(Based on consensus across all models and statistical methods)
1. ch6: Average rank 5.5 across all methods
2. ch0: Average rank 5.6 across all methods
3. ch3: Average rank 5.7 across all methods
4. ch2: Average rank 6.2 across all methods
5. ch8: Average rank 6.7 across all methods

DATASET CHARACTERISTICS:
- Training samples: 280 (WFB: 40, ADB: 60, UFB: 30)
- Unclassified cocoa bean samples: 10 (X1-X10)
- Sensor channels: 14 (ch0-ch13)
- Training classes: WFB, UFB, ADB
- Models analyzed: Random Forest, SVM, KNN, Neural Network, Naive Bayes

============================================================
FINAL CLASSIFICATION RESULTS
============================================================
Sample_ID Predicted_Class  Confidence          Model_Used
       X1             UFB    1.000000 Naive Bayes (Tuned)
       X2             UFB    1.000000 Naive Bayes (Tuned)
       X3             UFB    1.000000 Naive Bayes (Tuned)
       X4             UFB    1.000000 Naive Bayes (Tuned)
       X5             UFB    1.000000 Naive Bayes (Tuned)
       X6             WFB    1.000000 Naive Bayes (Tuned)
       X7             WFB    1.000000 Naive Bayes (Tuned)
       X8             WFB    1.000000 Naive Bayes (Tuned)
       X9             ADB    1.000000 Naive Bayes (Tuned)
      X10             ADB    0.994305 Naive Bayes (Tuned)

✅ Final classification results saved to: enose_run_11/data/final_classification_results.csv

============================================================
INTERPRETATION:
- WFB, ADB, UFB likely represent different types/qualities of cocoa beans
  * WFB: Well-Fermented Beans
  * ADB: Adequately-Fermented Beans
  * UFB: Under-Fermented Beans
- X1-X10 are unclassified cocoa bean samples
- The model predicts which known category each sample belongs to
- Higher confidence scores indicate more reliable predictions
- Consider validating results with domain experts
============================================================

✅ Comprehensive summary report saved to: enose_run_11/ANALYSIS_SUMMARY_REPORT_20251027_125133.txt

============================================================
ANALYSIS COMPLETE - ALL RESULTS SAVED
============================================================
📁 Results directory: enose_run_11
📊 Individual plots (25 files) saved in: enose_run_11/plots
📋 Data files (15 CSV files) saved in: enose_run_11/data
📝 Log files saved in: enose_run_11/logs
📄 Summary report: ANALYSIS_SUMMARY_REPORT_20251027_125133.txt
============================================================

🎉 Comprehensive analysis complete with confusion matrices!
   • Focus on 5 ML models: Random Forest, SVM, KNN, Neural Network, Naive Bayes
   • Comprehensive evaluation: Accuracy, Precision, Recall, Specificity, F1-Score, MCC
   • Confusion matrices visualization similar to research papers
   • Updated class labels: WFB (Well-Fermented), ADB (Adequately-Fermented), UFB (Under-Fermented)
   • Enhanced hyperparameter tuning for better model performance
   • Individual plots covering all aspects including class correlations
   • 15 CSV files with detailed results and comprehensive metrics
   Each file can be used separately in presentations or publications.
